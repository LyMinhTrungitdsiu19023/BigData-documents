{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6H-nouwaeyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39122463-58eb-4063-c4df-7ae05f8e6f56"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [1 InRelease 0 B/3,\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1,038 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,068 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,563 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,496 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,303 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,338 kB]\n",
            "Fetched 13.1 MB in 9s (1,441 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update"
      ],
      "metadata": {
        "id": "sGwEF300L3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3J63Uke14M"
      },
      "source": [
        "# `StringIndexer` and `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJgVwv-atT8"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(conf=SparkConf())\n",
        "spark = SparkSession(sparkContext=sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37etkvQa1bj"
      },
      "source": [
        "# Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJ5XVFGatqv",
        "outputId": "3324e84b-db8b-4527-b8d9-0344a9c2d725"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rom_jfRSa9dl"
      },
      "source": [
        "# StringIndexer\n",
        "\n",
        "`StringIndexer` maps a string column to a index column that will be treated as a categorical column by spark. **The indices start with 0 and are ordered by label frequencies**. If it is a numerical column, the column will first be casted to a string column and then indexed by  StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "1. Build the StringIndexer model: specify the input column and output column names.\n",
        "2. Learn the StringIndexer model: fit the model with your data.\n",
        "3. Execute the indexing: call the transform function to execute the indexing process.\n",
        "\n",
        "### Example: `StringIndex` column \"x1\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr74TF0Oattg",
        "outputId": "87e43103-ff34-45ed-c11d-45ba89da12ea"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+----------+\n",
            "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
            "+---+------+---+---+---+---+----------+\n",
            "|  a| apple|  1|2.4|  1|yes|       1.0|\n",
            "|  a|orange|  1|2.5|  0| no|       1.0|\n",
            "|  b|orange|  2|3.5|  1| no|       0.0|\n",
            "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
            "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
            "|  c| peach|  4|1.5|  1|yes|       2.0|\n",
            "+---+------+---+---+---+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09eR6hNbJ4X"
      },
      "source": [
        "From the result above, we can see that (a, b, c) in column x1 are converted to (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-aFv1K_bJ7H"
      },
      "source": [
        "# OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VhH9d-ybJ-N"
      },
      "source": [
        "**`OneHotEncoder`** converts each categories of a **StringIndexed** column to a `sparse vector`. Each sparse vector has **at most one single active elements** that indicate the category index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or9TkUeXatvv",
        "outputId": "f8f730e6-b103-4fd1-df8b-88c9971d03b5"
      },
      "source": [
        "df_ohe = df.select('x1')\n",
        "df_ohe.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| x1|\n",
            "+---+\n",
            "|  a|\n",
            "|  a|\n",
            "|  b|\n",
            "|  b|\n",
            "|  b|\n",
            "|  c|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm04i8AbTfY"
      },
      "source": [
        "### `StringIndex` column 'x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D21fB0GatzE",
        "outputId": "93953555-0e48-4539-f452-b833c04ab09c"
      },
      "source": [
        "df_x1_indexed = StringIndexer(inputCol='x1', outputCol='indexed_x1').fit(df_ohe).transform(df_ohe)\n",
        "df_x1_indexed.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| x1|indexed_x1|\n",
            "+---+----------+\n",
            "|  a|       1.0|\n",
            "|  a|       1.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  c|       2.0|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsHb_WbJbYUn"
      },
      "source": [
        "'x1' has three categories: 'a', 'b' and 'c',  which corresponding string indices 1.0, 0.0 and 2.0, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26L03v2bYXQ"
      },
      "source": [
        "### Mapping string indices to sparse vectors\n",
        "\n",
        "* Encoding format: 'string index': ['string indices vector size', 'index of string index in string indices vector', **1.0** ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFjx-kObYai"
      },
      "source": [
        "Here the string indices vector is `[0.0, 1.0, 2.0]`. Therefore, the mapping between string indices and sparse vectors are:\n",
        "* `0.0: [3, [0], [1.0]]`\n",
        "* `1.0: [3, [1], [1.0]]`\n",
        "* `2.0: [3, [2], [1.0]]`\n",
        "\n",
        "After we convert all sparse vectors to dense vectors, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9zDWhMat1R",
        "outputId": "21070cfd-bc50-49fd-818b-2d9d99bab28e"
      },
      "source": [
        "from pyspark.ml.linalg import DenseVector, SparseVector, DenseMatrix, SparseMatrix\n",
        "x = [SparseVector(3, {0: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {1: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {2: 1.0}).toArray()]\n",
        "\n",
        "import numpy as np\n",
        "np.array(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVbgtDBzbpTs"
      },
      "source": [
        "**The obtained matrix is exactly the matrix that we would use to represent our categorical variable in a statistical class**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDYvaXZEbpWq"
      },
      "source": [
        "### One more step to go\n",
        "\n",
        "`OneHotEncoder` by default will drop the last category. So the **string indices vector** becomes `[0.0, 1.0]`, and the mappings between string indices and sparse vectors are:\n",
        "\n",
        "* `0.0: [2, [0], [1.0]]`\n",
        "* `1.0: [2, [1], [1.0]]`\n",
        "* `2.0: [2, [], []]`\n",
        "\n",
        "We use a sparse vector that has **no active element**(basically all elements are 0's) to represent the last category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0oqTKkbpZK"
      },
      "source": [
        "# Verify\n",
        "\n",
        "### OneHotEncode column 'indexed_x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZuADf7iat8D"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UozxlA0cb3Nm",
        "outputId": "5f7658f1-9d4b-4945-c75b-2736eedbd492"
      },
      "source": [
        "OneHotEncoder(inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  c|       2.0|    (2,[],[])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YITbqgAgecCF"
      },
      "source": [
        "### Specify to not drop the last category\n",
        "\n",
        "If we choose to not drop the last category, we get the expected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6y1gArnb3Qw",
        "outputId": "43a8c7a4-4a59-4171-d9a7-6ad247c97aa3"
      },
      "source": [
        "OneHotEncoder(dropLast=False, inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  c|       2.0|(3,[2],[1.0])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw65oYrcb3Ss"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddY5V8VXfuxc"
      },
      "source": [
        "# Vector assembler\n",
        "\n",
        "## Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7SsboWb3Vz",
        "outputId": "77c0af20-8bcf-46f3-8a14-3e296c4bf51d"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6gUwqSJf8h4"
      },
      "source": [
        "# VectorAssembler\n",
        "\n",
        "To fit a ML model in pyspark, we need to combine all feature columns into one single column of vectors: the **featuresCol**. The `VectorAssembler` can be used to combine multiple **`OneHotEncoder` columns** and **other continuous variable columns** into one single column.\n",
        "\n",
        "The example below shows how to combine three OneHotEncoder columns and one numeric column into a **featureCol** column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7oT4vvf-WG"
      },
      "source": [
        "## StringIndex and OneHotEncode categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAhLCUkb3YB"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGqs4THGb3ak",
        "outputId": "1bc3c521-d729-42c2-8ba5-55c1d27dac4e"
      },
      "source": [
        "all_stages = [StringIndexer(inputCol=c, outputCol='idx_' + c) for c in ['x1', 'x2', 'x3']] + \\\n",
        "             [OneHotEncoder(inputCol='idx_' + c, outputCol='ohe_' + c) for c in ['x1', 'x2', 'x3']]\n",
        "all_stages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_9f95b9957997,\n",
              " StringIndexer_2b1fe319b5e0,\n",
              " StringIndexer_e26d8c753fd1,\n",
              " OneHotEncoder_b1ea378a61bd,\n",
              " OneHotEncoder_e2b21696726d,\n",
              " OneHotEncoder_b07ab84e0d20]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A92-NF0Pat-m",
        "outputId": "f82b394a-3705-4a5f-8297-3125d98c9144"
      },
      "source": [
        "df_new = Pipeline(stages=all_stages).fit(df).transform(df)\n",
        "df_new.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "| x1|    x2| x3| x4| y1| y2|idx_x1|idx_x2|idx_x3|       ohe_x1|       ohe_x2|       ohe_x3|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "|  a| apple|  1|2.4|  1|yes|   1.0|   2.0|   1.0|(2,[1],[1.0])|    (2,[],[])|(2,[1],[1.0])|\n",
            "|  a|orange|  1|2.5|  0| no|   1.0|   0.0|   1.0|(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|  b|orange|  2|3.5|  1| no|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b|orange|  2|1.4|  0|yes|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b| peach|  2|2.1|  0|yes|   0.0|   1.0|   0.0|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
            "|  c| peach|  4|1.5|  1|yes|   2.0|   1.0|   2.0|    (2,[],[])|(2,[1],[1.0])|    (2,[],[])|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K2_vP5qgOhf"
      },
      "source": [
        "## Assemble feature columns into one single **feacturesCol** with **`VectorAssembler`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "023kdI6bgK8L",
        "outputId": "ba429ec4-2d18-4508-8f97-26959f23f49b"
      },
      "source": [
        "df_assembled = VectorAssembler(inputCols=['ohe_x1', 'ohe_x2', 'ohe_x3', 'x4'], outputCol='featuresCol')\\\n",
        "    .transform(df_new)\\\n",
        "    .drop('idx_x1', 'idx_x2', 'idx_x3')\n",
        "df_assembled.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|x1 |x2    |x3 |x4 |y1 |y2 |ohe_x1       |ohe_x2       |ohe_x3       |featuresCol                  |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|a  |apple |1  |2.4|1  |yes|(2,[1],[1.0])|(2,[],[])    |(2,[1],[1.0])|(7,[1,5,6],[1.0,1.0,2.4])    |\n",
            "|a  |orange|1  |2.5|0  |no |(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|[0.0,1.0,1.0,0.0,0.0,1.0,2.5]|\n",
            "|b  |orange|2  |3.5|1  |no |(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,3.5]|\n",
            "|b  |orange|2  |1.4|0  |yes|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,1.4]|\n",
            "|b  |peach |2  |2.1|0  |yes|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|[1.0,0.0,0.0,1.0,1.0,0.0,2.1]|\n",
            "|c  |peach |4  |1.5|1  |yes|(2,[],[])    |(2,[1],[1.0])|(2,[],[])    |(7,[3,6],[1.0,1.5])          |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5loOVu0gTb_"
      },
      "source": [
        "## Convert sparse vectors in featuresCol to dense vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXnLcxQgLZ7"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoHGEeBLgLgw",
        "outputId": "d83fc289-eca5-4484-d601-f3e45ea412ad"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: x['featuresCol']).take(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SparseVector(7, {1: 1.0, 5: 1.0, 6: 2.4}),\n",
              " DenseVector([0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtYLdUhogZmB",
        "outputId": "3a7e30e9-44b1-4f7b-d430-212ad776f76f"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: list(x['featuresCol'].toArray())).take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.4],\n",
              " [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4],\n",
              " [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.1]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef9g5bCrgbnn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}